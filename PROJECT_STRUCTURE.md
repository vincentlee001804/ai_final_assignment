# Project Structure and Execution

## ğŸ“ Project Structure

```
finalassignment/
â”œâ”€â”€ archive/                    # Raw Kaggle dataset (source data)
â”‚   â””â”€â”€ [original dataset structure]
â”‚
â”œâ”€â”€ data/                       # Processed dataset (70/15/15 split)
â”‚   â”œâ”€â”€ train/ (benign, malignant)
â”‚   â”œâ”€â”€ val/ (benign, malignant)
â”‚   â””â”€â”€ test/ (benign, malignant)
â”‚
â”œâ”€â”€ scripts/                    # Main execution scripts
â”‚   â”œâ”€â”€ split_dataset.py        # Creates data/train, data/val, data/test from archive/
â”‚   â”œâ”€â”€ train_all_models.py    # Train all models (with fixed hyperparameters)
â”‚   â”œâ”€â”€ evaluate_all_models.py # Evaluate all models on test set
â”‚   â”œâ”€â”€ compare_models.py      # Compare and recommend best model
â”‚   â”œâ”€â”€ run_complete_pipeline.py
â”‚   â””â”€â”€ generate_report_data.py
â”‚
â”œâ”€â”€ models/                     # Individual model training scripts
â”‚   â”œâ”€â”€ train_alexnet.py
â”‚   â”œâ”€â”€ train_googlenet.py
â”‚   â”œâ”€â”€ train_resnet18.py
â”‚   â”œâ”€â”€ train_resnet50.py
â”‚   â”œâ”€â”€ train_resnet101.py
â”‚   â”œâ”€â”€ train_densenet169.py
â”‚   â”œâ”€â”€ train_mobilenet_v2.py
â”‚   â”œâ”€â”€ train_mobilenet_v3_small.py
â”‚   â”œâ”€â”€ train_mobilenet_v3_large.py
â”‚   â”œâ”€â”€ train_vgg16.py
â”‚   â”œâ”€â”€ train_vgg19.py
â”‚   â””â”€â”€ model.py
â”‚
â”œâ”€â”€ utils/                      # Utility functions and helpers
â”‚   â”œâ”€â”€ train.py               # Data loaders, training and validation loops
â”‚   â”œâ”€â”€ evaluate.py            # Evaluation functions
â”‚   â”œâ”€â”€ classify.py             # Classification utilities
â”‚   â”œâ”€â”€ check_gpu.py            # GPU verification
â”‚   â””â”€â”€ [checkpoint saving/loading functions]
â”‚
â”œâ”€â”€ samples/                    # Reference sample code
â”‚   â”œâ”€â”€ sampletrain.py
â”‚   â”œâ”€â”€ samplemodel.py
â”‚   â””â”€â”€ sampleclassify.py
â”‚
â”œâ”€â”€ docs/                       # Documentation
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ ASSIGNMENT_CHECKLIST.md
â”‚   â”œâ”€â”€ EARLY_STOPPING_INFO.md
â”‚   â”œâ”€â”€ FOLDER_STRUCTURE.md
â”‚   â”œâ”€â”€ HYPERPARAMETER_TUNING_STATUS.md
â”‚   â””â”€â”€ Assignment.pdf
â”‚
â”œâ”€â”€ results/                    # Output files (auto-generated)
â”‚   â”œâ”€â”€ trained_models/         # Model checkpoints
â”‚   â”‚   â”œâ”€â”€ {model}_best.pt     # Best model (lowest validation loss)
â”‚   â”‚   â”œâ”€â”€ {model}_last.pt     # Final model (last epoch)
â”‚   â”‚   â””â”€â”€ {model}_history.json
â”‚   â”œâ”€â”€ evaluation_results.json # All evaluation metrics
â”‚   â”œâ”€â”€ model_comparison.csv    # Comparison table
â”‚   â”œâ”€â”€ roc_curves/             # ROC curve plots
â”‚   â”‚   â””â”€â”€ {model}_roc.png
â”‚   â”œâ”€â”€ comparison_plots/       # Visualization plots
â”‚   â”œâ”€â”€ hyperparameter_results/ # Hyperparameter tuning results
â”‚   â””â”€â”€ best_model_recommendation.txt
â”‚
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ class_name.txt              # Generated class names file
â”œâ”€â”€ README.md                   # Main README
â”œâ”€â”€ ORGANIZATION_SUMMARY.md     # Organization details
â”œâ”€â”€ QUICK_START.md              # Quick reference
â””â”€â”€ PROJECT_STRUCTURE.md        # This file
```

## ğŸ”„ Execution Flow

### 1. Data Preparation

**Step:** Split raw dataset into train/val/test splits

```bash
python scripts/split_dataset.py
```

**What it does:**
- Reads raw Kaggle dataset from `archive/` folder
- Splits into 70% train, 15% validation, 15% test
- Maintains class balance across splits
- Creates `data/train/`, `data/val/`, and `data/test/` folders

**Output:**
- `data/train/` (benign, malignant)
- `data/val/` (benign, malignant)
- `data/test/` (benign, malignant)

### 2. Model Training

**Option A: Train all models**
```bash
python scripts/train_all_models.py --model all
```

**Option B: Train individual models**
```bash
python models/train_alexnet.py
python models/train_resnet50.py
# ... etc
```

**What happens:**
- Each `train_*.py` script trains one model
- Uses data loaders and training/validation loops from `utils/train.py`
- Saves checkpoints using checkpoint saving functions
- Implements early stopping to prevent overfitting

**Output:**
- `results/trained_models/{model}_best.pt` - Best model (lowest validation loss)
- `results/trained_models/{model}_last.pt` - Final model (last epoch)
- `results/trained_models/{model}_history.json` - Training history

### 3. Model Evaluation

**Evaluate all models:**
```bash
python scripts/evaluate_all_models.py --model all
```

**Evaluate specific model:**
```bash
python scripts/evaluate_all_models.py --model resnet50
```

**What it does:**
- Loads all `*_best.pt` files from `results/trained_models/`
- Evaluates each model on `data/test/`
- Calculates comprehensive metrics:
  - Accuracy
  - Precision (per class)
  - Recall (per class)
  - True Negative Rate (TNR/Specificity)
  - Macro Precision
  - Macro Recall
  - ROC Curve and AUC
  - Confusion Matrix

**Output:**
- `results/evaluation_results.json` - All evaluation metrics (JSON format)
- `results/roc_curves/{model}_roc.png` - Individual ROC curve for each model

**Note:** The evaluation script currently outputs JSON. If you need CSV format, you can convert the JSON or modify the script to output CSV directly.

### 4. Model Comparison

```bash
python scripts/compare_models.py
```

**Output:**
- `results/model_comparison.csv` - Detailed comparison table
- `results/comparison_plots/model_comparison.png` - Visualization
- `results/best_model_recommendation.txt` - Final recommendation

### 5. Complete Pipeline

Run everything in sequence:
```bash
python scripts/run_complete_pipeline.py
```

This executes:
1. Train all models with fixed hyperparameters (Task 3)
2. Evaluate all models (Task 5)
3. Compare and recommend (Task 6)
4. Generate report data (Task 7)

## ğŸ“ Key Components

### Data Loaders and Training Utilities

**Location:** `utils/train.py` and `utils/evaluate.py`

**Functions:**
- Data loaders for train/val/test sets
- Training loop with forward pass, loss calculation, backpropagation
- Validation loop with metrics calculation
- Checkpoint saving/loading functions
- Early stopping implementation

### Model Checkpoints

**Format:** PyTorch `.pt` files (state_dict)

**Files saved:**
- `{model}_best.pt` - Best model based on validation loss
- `{model}_last.pt` - Model from final epoch
- `{model}_history.json` - Training metrics history

**Location:** `results/trained_models/`

### Evaluation Metrics

**Binary Classification Metrics:**
- Accuracy
- Precision (per class: benign, malignant)
- Recall (per class: benign, malignant)
- True Negative Rate (TNR/Specificity)
- Macro Precision (average)
- Macro Recall (average)
- ROC Curve and AUC
- Confusion Matrix

**Output Format:**
- JSON: `results/evaluation_results.json`
- CSV: `results/model_comparison.csv` (via compare_models.py)
- Plots: `results/roc_curves/{model}_roc.png`

## ğŸš€ Quick Start

1. **Prepare data:**
   ```bash
   python scripts/split_dataset.py
   ```

2. **Train models:**
   ```bash
   python scripts/train_all_models.py --model all
   ```

3. **Evaluate models:**
   ```bash
   python scripts/evaluate_all_models.py --model all
   ```

4. **Compare and get recommendation:**
   ```bash
   python scripts/compare_models.py
   ```

Or run everything at once:
```bash
python scripts/run_complete_pipeline.py
```

## ğŸ“Š Output Files Summary

| File/Folder | Description |
|------------|-------------|
| `results/trained_models/{model}_best.pt` | Best model checkpoint |
| `results/trained_models/{model}_last.pt` | Final epoch checkpoint |
| `results/trained_models/{model}_history.json` | Training history |
| `results/evaluation_results.json` | All evaluation metrics |
| `results/model_comparison.csv` | Comparison table |
| `results/roc_curves/{model}_roc.png` | ROC curves per model |
| `results/comparison_plots/model_comparison.png` | Comparison visualization |
| `results/best_model_recommendation.txt` | Best model recommendation |

