# Skin Cancer Classification Assignment

Complete implementation for training and evaluating multiple deep learning models for skin cancer classification.

## ğŸ“ Folder Structure

```
finalassignment/
â”œâ”€â”€ data/                    # Dataset (70/15/15 split)
â”‚   â”œâ”€â”€ train/ (benign, malignant)
â”‚   â”œâ”€â”€ val/ (benign, malignant)
â”‚   â””â”€â”€ test/ (benign, malignant)
â”‚
â”œâ”€â”€ scripts/                 # Main scripts
â”‚   â”œâ”€â”€ train_all_models.py
â”‚   â”œâ”€â”€ evaluate_all_models.py
â”‚   â”œâ”€â”€ compare_models.py
â”‚   â”œâ”€â”€ run_complete_pipeline.py
â”‚   â””â”€â”€ generate_report_data.py
â”‚
â”œâ”€â”€ models/                  # Individual model training scripts
â”‚   â”œâ”€â”€ train_alexnet.py
â”‚   â”œâ”€â”€ train_googlenet.py
â”‚   â”œâ”€â”€ train_resnet18.py
â”‚   â”œâ”€â”€ train_resnet50.py
â”‚   â”œâ”€â”€ train_resnet101.py
â”‚   â”œâ”€â”€ train_densenet169.py
â”‚   â”œâ”€â”€ train_mobilenet_v2.py
â”‚   â”œâ”€â”€ train_mobilenet_v3_small.py
â”‚   â”œâ”€â”€ train_mobilenet_v3_large.py
â”‚   â”œâ”€â”€ train_vgg16.py
â”‚   â”œâ”€â”€ train_vgg19.py
â”‚   â””â”€â”€ model.py
â”‚
â”œâ”€â”€ utils/                   # Utility scripts
â”‚   â”œâ”€â”€ check_gpu.py
â”‚   â”œâ”€â”€ classify.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ train.py
â”‚
â”œâ”€â”€ samples/                 # Sample code (reference)
â”‚   â”œâ”€â”€ sampletrain.py
â”‚   â”œâ”€â”€ samplemodel.py
â”‚   â””â”€â”€ sampleclassify.py
â”‚
â”œâ”€â”€ docs/                    # Documentation
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ ASSIGNMENT_CHECKLIST.md
â”‚   â””â”€â”€ Assignment.pdf
â”‚
â””â”€â”€ results/                 # Output files (auto-generated)
    â”œâ”€â”€ trained_models/
    â”œâ”€â”€ hyperparameter_results/
    â”œâ”€â”€ comparison_plots/
    â””â”€â”€ roc_curves/
```

## ğŸš€ Quick Start

### Installation

```bash
pip install -r requirements.txt
```

### Verify GPU (Optional)

```bash
python utils/check_gpu.py
```

## ğŸ“‹ Usage

### Option 1: Run Complete Pipeline (Recommended)

Run all steps automatically in the correct order:
```bash
python scripts/run_complete_pipeline.py
```

**This will execute:**
1. Task 3: Train all 11 models (with fixed hyperparameters)
2. Task 5: Evaluate all models
3. Task 6: Compare and recommend best model
4. Task 7: Generate report data

### Option 2: Run Steps Individually

#### Step 1: Train All Models (Task 3)

**Train all 11 models:**
```bash
python scripts/train_all_models.py --model all
```

**Train a specific model:**
```bash
python scripts/train_all_models.py --model resnet50
```

**Or use individual scripts:**
```bash
python models/train_alexnet.py
python models/train_resnet50.py
# ... etc
```

**Output:**
- `results/trained_models/`: All trained model checkpoints (`{model_name}_best.pt`)
- `results/trained_models/`: Training history JSON files

#### Step 2: Evaluate All Models (Task 5)

**Evaluate all models on test set:**
```bash
python scripts/evaluate_all_models.py --model all
```

**Evaluate a specific model:**
```bash
python scripts/evaluate_all_models.py --model resnet50
```

**Metrics Calculated (for binary classification):**
- Accuracy
- Precision (per class)
- Recall (per class)
- True Negative Rate (TNR/Specificity)
- Macro Precision
- Macro Recall
- ROC Curve and AUC

**Output:**
- `results/evaluation_results.json`: All evaluation metrics
- `results/roc_curves/`: ROC curve plots for each model
- Console output with detailed metrics

#### Step 3: Compare Models (Task 6)

Compare all models and get recommendation:
```bash
python scripts/compare_models.py
```

**Output:**
- `results/model_comparison.csv`: Detailed comparison table
- `results/comparison_plots/model_comparison.png`: Visualization plots
- `results/best_model_recommendation.txt`: Final recommendation

#### Step 4: Generate Report Data (Task 7)

Generate formatted data for your technical report:
```bash
python scripts/generate_report_data.py
```

**Output:**
- `results/report_data.txt`: Formatted data for report sections

## ğŸ“Š Models Implemented

1. AlexNet
2. GoogleNet
3. ResNet18
4. ResNet50
5. ResNet101
6. DenseNet169
7. MobileNetV2
8. MobileNetV3 Small
9. MobileNetV3 Large
10. VGG16
11. VGG19

## ğŸ“ˆ Evaluation Metrics

For binary classification (benign vs malignant):
- âœ… Accuracy
- âœ… Recall (Sensitivity)
- âœ… True Negative Rate (TNR/Specificity)
- âœ… Precision
- âœ… ROC Curve and AUC

## ğŸ“ Output Files

All outputs are saved in `results/` folder:
- `results/trained_models/` - Model checkpoints
- `results/evaluation_results.json` - All metrics
- `results/model_comparison.csv` - Comparison table
- `results/comparison_plots/` - Visualizations
- `results/roc_curves/` - ROC curves
- `results/best_model_recommendation.txt` - Recommendation
- `results/report_data.txt` - Report data

## ğŸ“ Assignment Tasks

âœ… Task 1: Dataset (data/)  
âœ… Task 2: Data split (70/15/15)  
âœ… Task 3: Train all 11 models (with fixed hyperparameters)  
âœ… Task 5: Evaluation with all metrics  
âœ… Task 6: Compare and recommend  
âœ… Task 7: Report data generation  

## ğŸ”§ Notes

- All scripts follow `samples/sampletrain.py` pattern
- Uses SGD with momentum (0.9)
- Simple transforms (Resize + ToTensor)
- Early stopping prevents overfitting
- All outputs organized in `results/` folder

## ğŸ“š Documentation

- `docs/README.md` - Full documentation
- `docs/ASSIGNMENT_CHECKLIST.md` - Task checklist
- `QUICK_START.md` - Quick reference guide
- `ORGANIZATION_SUMMARY.md` - Folder organization details

